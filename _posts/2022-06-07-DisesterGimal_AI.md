---
publish: true
title: "학교 - 4학년 1학기 파멸의 기말고사 - 인공지능의 역습"
categories:
  - PTU
tags:
  - 인공지능(4학년-1학기)
---

![image](https://user-images.githubusercontent.com/60723373/172381488-c5f1e668-457f-40e9-9177-a7ec51df95de.png)

### 해당 사진은 2022년 평택대학교 4학년 1학기 인공지능 과목을 수강하시는 모든 학우들을 위해 바칩니다.

1. 교수님이 제공해준 기말고사 준비도 테스트 문제(?)의 답을 먼저 적어넣었음.
2. 9 ~ 14주 까지에서 나올만한 것들을 추려서 적었음.

## 1. 기.준.테 문제 : 답과 해설

1. ()은 나와 유사한 성향을 갖고 있는 사람들이 좋아하는 것은 나도 좋아할 가능성이 높을 것이라는 가정에 기반한 알고리즘이다.

2. ()은 지능을 가진 동물이 시행착오를 통해 학습하는 방법을 모델링한 것이다.

3. ()을 제안한 이안 굿펠로(Ian Goodfellow)는 ()을 경찰과 위조 지폐범 사이의 게임에 비유한다.

4. k-평균 알고리즘에서 k는 몇 개의 무리로 ()할 것인지를 나타내는 것이다.

5. 강화학습에서 유일하게 참조하는 것은 에이전트가 행동을 취한 후 얻는 ()이다.

6. 고전적인 방법으로 자연어를 처리하는 것에 한계를 느껴 암흑기가 도래하였으나 이후 ()으로 해결하였다.

7. 깊이가 깊은 심층신경망에서는 역전파 알고리즘이 입력층으로 전달됨에 따라 그래디언트가 점점 작아져 결국 가중치 매개변수가 업데이트 되지 않는 경우가 발생하게 된다. 이러한 문제를 ()이라고 한다.

8. 다층 신경망은 중간에 은닉층을 추가함으로써 () 문제를 해결한다.

9. 딥러닝은 ()를 해결하기 위해 랜덤하게 신경망 연결을 끊어 일반화 능력을 향상시켰다.

10. 딥러닝은 ‘논리적인 접근 방식’에서 () 접근 방식으로 전환한 것이다.

11. 비지도학습은 입력된 데이터를 받아 컴퓨터가 스스로 데이터들의 ()을 찾아내 학습한다.

12. 비지도학습의 목표는 레이블 없이 확보된 데이터의 특성을 분석해서 유사한 특성을 가진 값끼리 ()하는 것이다.

13. 인공지능 분야에서 가장 어려운 분야가 () 분야이다.

14. 자연어처리에서 앞뒤 데이터 간에 연관성이 있는 데이터셋에 사용하는 모델인 ()을 사용하여 시계열적 시간 특성이 포함된 동적인 언어 데이터 처리가 가능해졌다.

15. 지도학습의 목적은 입력 데이터와 결과값을 이용해 특정한 타깃을 ( )하는 것이다.

16. 학습된 알고리즘이 예측하는 결과값이 이산값이면 ( ) 문제이고, 연속 값이면 () 문제에 해당된다.

### 아래는 정답입니다.

1. 협업 필터링
2. 강화학습
3. 생성적 적대 신경망(Generative Adversarial Network), GAN
4. 클러스터링
5. 보상
6. 딥러닝
7. 그래디언트 소실
8. XOR
9. 과대적합 문제
10. 실험적
11. 특징
12. 그룹화
13. 자연어 처리
14. RNN
15. 예측
16. 분류, 회귀

## PPT 중 나올 만한 부분 키워드로 정리

### 7장

- 지도학습 : 문제(Feature)와 정답(Label)이 있는 학습데이터(Training Set)를 컴퓨터에 학습
- 지도학습의 목적은 특정한 타깃을 예측하는 것
- 예측하는 결과값이 **이산값 => 분류 문제**
- 예측하는 결과값이 **연속값 => 회귀 문제**
- 회귀 : 독립변수와 종속변수의 관계를 설명할 때 가장 많이 쓰는 모델
- **선형 회귀 모델** : **분류하는 요인의 수가 적을 때 활용**하기 용이
- 분류에는 의사결정 트리, 로지스틱 회귀
- 회귀에는 회귀 트리(랜덤 포레스트), 선형 회귀
- k - 최근접 이웃 : 최근접 이웃을 찾음
- 로지스틱 회귀 : 시그모이드 함수를 씀
- 서포트 백터 머신 : 데이터들의 경계 중 가장 큰 폭을 가진 경계를 찾음
- 나이트 베이즈 : 복잡한 베이즈 정리를 간단한 조건부 확률 방법을 이용해 분류하는 알고리즘
- 의사결정 트리 : 특정 기준에 따라 데이터를 구분하는 학습 모델
- 랜덤 포레스트 : 분류, 회귀분석 등에 사용되는 앙상블 학습 방법의 일종
- 신경망/딥러닝 : 인간의 신경 모델을 모방한 학습 방법. 뉴런의 가중값 구함.

- 의사 결정 트리

  - 정보 획득량 : 어떤 사건이 얼마만큼의 정보를 줄 수 있는지를 수치화 => 정보 함수와 엔트로피 필요
  - 엔트로피 : 무질서도를 정량화해 나타낸 값
  - 의사결정 트리는 정보 함수와 엔트로피를 사용해 나타낸 정보 획득량을 최대화한느 순서로 배치하는 것. 중요도 값 계산을 위해 기계학습 사용.

- k-NN, k-최근접 이웃 알고리즘 : 새로운 데이터가 주어질 때 기존 데이터 가운데 **가장 가까운 k개 이웃의 정보로 새로운 데이터 예측**.
- k는 작은 값이 좋음. => 크면 분류 못할수도 있음.
- k값은 기본적으로 홀수
- 유클리드 거리로 계산함.

### 8장

- 비지도학습 : 훈련데이터에 레이블이 없음. 스스로 데이터의 특징 찾음.
- 목표 : 유사한 속성을 가진 값끼리 그룹화하는 것
- 알고리즘 종류

  - 군집
    - k-평균(k-means) : 주어진 데이터를 지정된 클러스터 갯수(k)로 그룹핑
    - 계층 군집 분석(HCA) : 하나의 case가 될 때까지 군집을 묶는 클러스터링. 군집 간의 거리를 기반으로 클러스터링을 하는 알고리즘. 군집 수 안 정해도 됨.
  - 시각화와 차원 축소 : 주성분 분석, 커널, 지역적 선형 임베딩, -SNE

- k-평균 알고리즘
  - 클러스터 : 유사한 특성을 가진 데이터끼리의 묶음
  - 클러스터링 : 어떤 데이터들이 주어질 때, 그 데이터들을 클러스터로 무리지어 주는 것
  - 센트로이드 : 클러스터의 중심
  - k : 몇 개의 무리로 클러스터링할 것인지 나타냄.
  - 차원 축소 : 상관관계가 있는 여러 특징을 하나로 합치는 것
  - 특징 추출 : 두 가지 특성을 어떤 사건의 원인을 나타내는 하나의 특성으로 합침.
- 협업 필터링 : 내가 좋아하는건 남도 좋아할 가능성이 높다고 생각한 알고리즘.

  - 협업 필터링의 종류
    - 사용자 기반 : **좋아하는 성향이 유사한 사용자들을 같은 그룹으로 묶고 ** 그 그룹에서 선호하는 상품을 추천
    - 아이템 기반 : **사용자가 이전에 구매한 아이템을 기반**으로 연관성 있는 상품 추천
  - 장점 : 직관적, 신뢰도 높음
  - 단점
    - 콜드 스타트 문제 : 데이터 의존도 높음 => 새로운 페턴에 대한 추천 어려움.
    - First-Rater 문제 : 기존에 구매가 이뤄지지 않은 상품은 추천이 안 일어남.
  - 단점을 보완하기 위해 콘텐츠 기반 필터링이 함께 쓰임!
  - 전통적 알고리즘
    - 협업 필터링
    - 콘텐츠 기반 필터링
  - 최신 알고리즘
    - 모델 기반 협력 필터링
    - 딥러닝 기반 필터링

- GAN, 생성적 적대 신경망 : 이안 굿펠로는 GAN을 경찰과 위조 지폐범 사이의 게임에 비유
  - 저해상도 이미지를 고해상도 개선, 음성 복원, 게임 배경화면 생성...

### 9장

- 강화학습 : 보상을 최대화 하기 위해 스스로 선택하는 학습, 동물의 학습법

  - 에이전트 : 지정한 작업을 수행하기 위해 훈련하는 프로그램
  - 환경 : 에이전트가 조치를 수행하는 실제 또는 가상 세계
  - 동작 : 에이전트에 의한 조치로 환경의 상태가 변경
  - 보상 : 긍적적이거나 부정적일 수 있는 행동의 평가 값
  - 지도, 비지도는 정적이며 강화학습은 동적임
  - 강화학습은 명백한 정답이 없음
  - 실시간 탐색이 필요함
  - 지도학습은 단일 결정 프로세스, 강화학습은 다중 결정 프로세스가 필요.
  - 강화학습의 대표적인 기법 : 확률 기반 정책 그레이디언트, Q-러닝
  - 마르코프 결정 프로세스(MDP) : 순차적으로 내려야 하는 문제를 정의할 때 사용.
    - Q(At) : 결정하는 프로세스의 각 상태
    - Q(At)의 값 : 보상이 높은 쪽으로 이동하는 정책
  - 마르코프 체인 : 마르코프 성질을 지닌 이산 확률 과정
  - 미르코프 모델 : 상태 => 상태 전이 확률 => 상태 전이도 순서

- Q-Learning : R-matrix를 이용. 그를 위해서는 각 이동 경로를 학습하기 위한 Q-matrix 만듬.

### 10장

- 인공신경망 : 사람 뇌 신경세포를 복잡한 스위치들이 연결된 네트워크로 표현
- 단층 퍼셉트론 : Y = Activity Function(X \* W + b)
- W = 가중값, b = (편향 Bias)
- 단층 퍼셉트론의 경우 AND, OR이 가능하나, XOR이 안되어 인공지능 1차 겨울이 도래함
- 다층 퍼셉트론 : 중간에 은닉층을 추가해 XOR 문제 해결
  - S자모양의 시그모이드 사용
  - 신경망 개념을 도입하면 가중값과 편향값을 자동으로 학습해 적절한 값 대입.
  - 계단 함수 : 출력 값이 0과 1 나옴
  - 시그모이드 : s자 모양의 곡선이 특징임
  - ReLu(렐루) 함수 : 0이하의 값은 무시하고 0을 넘으면 그대로 출력
  - 가중값 조절 방법은 아래와 같다 - 경사 하강법 : 가중값 w가 기울기 방향으로 조금씩 이동. 오차가 작아질 때 까지 반복 -다층 신경망의 문제점 - 모든 노드마다 계산하기에 느림 - 그레디언트 소실(위 문제 참조) - 과대 적합(과도한 학습은 새로운 입력에 정확성이 떨어짐)
  - 문제 해결법
    - 역전파 알고리즘 : 오차의 기울기를 한 번만 미분하고, 그 결과를 뒤로 전파하면서 재사용
- 딥러닝(심층 신경망)
  - 기존 신경망에서 은닉층과 출력층이 2개 이상으로 증가.
  - 계산량이 지수적으로 증가했으나, 여러 가지 분류가 가능.
  - 손실함수 : 신경망의 출력값이 기대하는 것 보다 얼마나 벗어났는지 측정하며 관찰 필요
  - 훈련을 반복하며, 실험적 접근방식을 가지고 있음.
